{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mskang/.conda/envs/server/lib/python3.9/site-packages/xgboost/compat.py:93: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys\n",
    "import sktime\n",
    "import tqdm as tq\n",
    "import xgboost as xgb\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.utils.plotting import plot_series\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 포트 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n",
      "Current : 0\n",
      "Count : 1\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('device :', device)\n",
    "print('Current :', torch.cuda.current_device())\n",
    "print('Count :', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>date_time</th>\n",
       "      <th>power</th>\n",
       "      <th>prec</th>\n",
       "      <th>wind</th>\n",
       "      <th>hum</th>\n",
       "      <th>temp</th>\n",
       "      <th>day</th>\n",
       "      <th>week</th>\n",
       "      <th>day_hour_mean</th>\n",
       "      <th>day_hour_std</th>\n",
       "      <th>holiday</th>\n",
       "      <th>sin_time</th>\n",
       "      <th>cos_time</th>\n",
       "      <th>THI</th>\n",
       "      <th>CDH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-01 00:00:00</td>\n",
       "      <td>1085.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>42.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1774.744615</td>\n",
       "      <td>517.982222</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.6576</td>\n",
       "      <td>-5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-01 01:00:00</td>\n",
       "      <td>1047.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1687.347692</td>\n",
       "      <td>500.769931</td>\n",
       "      <td>1</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>47.7625</td>\n",
       "      <td>-11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-01 02:00:00</td>\n",
       "      <td>974.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>45.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1571.483077</td>\n",
       "      <td>465.227458</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>47.2225</td>\n",
       "      <td>-17.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-01 03:00:00</td>\n",
       "      <td>953.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>48.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1522.153846</td>\n",
       "      <td>436.601091</td>\n",
       "      <td>1</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>44.7856</td>\n",
       "      <td>-25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-01 04:00:00</td>\n",
       "      <td>986.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>43.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1506.793846</td>\n",
       "      <td>405.518091</td>\n",
       "      <td>1</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>49.0061</td>\n",
       "      <td>-30.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num            date_time    power  prec  wind   hum  temp  day  week  \\\n",
       "0    1  2022-06-01 00:00:00  1085.28   0.0   0.9  42.0  18.6    2    22   \n",
       "1    1  2022-06-01 01:00:00  1047.36   0.0   1.1  45.0  18.0    2    22   \n",
       "2    1  2022-06-01 02:00:00   974.88   0.0   1.5  45.0  17.7    2    22   \n",
       "3    1  2022-06-01 03:00:00   953.76   0.0   1.4  48.0  16.7    2    22   \n",
       "4    1  2022-06-01 04:00:00   986.40   0.0   2.8  43.0  18.4    2    22   \n",
       "\n",
       "   day_hour_mean  day_hour_std  holiday  sin_time  cos_time      THI   CDH  \n",
       "0    1774.744615    517.982222        1  0.000000  1.000000  49.6576  -5.4  \n",
       "1    1687.347692    500.769931        1  0.258819  0.965926  47.7625 -11.4  \n",
       "2    1571.483077    465.227458        1  0.500000  0.866025  47.2225 -17.7  \n",
       "3    1522.153846    436.601091        1  0.707107  0.707107  44.7856 -25.0  \n",
       "4    1506.793846    405.518091        1  0.866025  0.500000  49.0061 -30.6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전처리 한 파일 불러오기\n",
    "train = pd.read_csv('./data/xgboost/train_preprocessed.csv', index_col=0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/xgboost/test_preprocessed.csv', index_col = 0)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SMAPE loss function\n",
    "def SMAPE(true, pred):\n",
    "    return np.mean((np.abs(true-pred))/(np.abs(true) + np.abs(pred))) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eta(learning rate) : 0.01 로 했을 때보다 0.05로 했을 때 결과가 좋게 나옴  \n",
    "최적의 eta 값을 찾기 위해 바꿔가며 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/mskang/.conda/envs/server/lib/python3.9/site-packages/xgboost/data.py:173: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 168 candidates, totalling 1680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [12:33<20:43:48, 753.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.9, 'eta': 0.07, 'max_depth': 5, 'min_child_weight': 6, 'n_estimators': 100, 'subsample': 0.8}\n",
      "building1|| SMAPE : 1.5148426683419873\n",
      "Fitting 10 folds for each of 168 candidates, totalling 1680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mskang/.conda/envs/server/lib/python3.9/site-packages/xgboost/data.py:173: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "  2%|▏         | 2/100 [24:48<20:13:07, 742.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eta': 0.07, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
      "building2|| SMAPE : 1.0043933693459162\n",
      "Fitting 10 folds for each of 168 candidates, totalling 1680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mskang/.conda/envs/server/lib/python3.9/site-packages/xgboost/data.py:173: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "  3%|▎         | 3/100 [37:04<19:55:42, 739.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.9, 'eta': 0.07, 'max_depth': 4, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "building3|| SMAPE : 2.891783319865605\n",
      "Fitting 10 folds for each of 168 candidates, totalling 1680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mskang/.conda/envs/server/lib/python3.9/site-packages/xgboost/data.py:173: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "  4%|▍         | 4/100 [48:30<19:09:16, 718.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eta': 0.07, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 100, 'subsample': 0.9}\n",
      "building4|| SMAPE : 1.6763882841392252\n",
      "Fitting 10 folds for each of 168 candidates, totalling 1680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mskang/.conda/envs/server/lib/python3.9/site-packages/xgboost/data.py:173: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "  5%|▌         | 5/100 [1:00:11<18:47:17, 711.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eta': 0.07, 'max_depth': 5, 'min_child_weight': 7, 'n_estimators': 100, 'subsample': 0.8}\n",
      "building5|| SMAPE : 1.2612768531058638\n",
      "Fitting 10 folds for each of 168 candidates, totalling 1680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mskang/.conda/envs/server/lib/python3.9/site-packages/xgboost/data.py:173: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "  6%|▌         | 6/100 [1:11:49<18:28:11, 707.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eta': 0.07, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "building6|| SMAPE : 1.3886586398389134\n",
      "Fitting 10 folds for each of 168 candidates, totalling 1680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mskang/.conda/envs/server/lib/python3.9/site-packages/xgboost/data.py:173: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "  7%|▋         | 7/100 [1:24:14<18:35:36, 719.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eta': 0.07, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 100, 'subsample': 0.9}\n",
      "building7|| SMAPE : 1.6430703418174504\n",
      "Fitting 10 folds for each of 168 candidates, totalling 1680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mskang/.conda/envs/server/lib/python3.9/site-packages/xgboost/data.py:173: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "  8%|▊         | 8/100 [1:34:35<17:35:28, 688.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eta': 0.07, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "building8|| SMAPE : 1.9882368365485605\n",
      "Fitting 10 folds for each of 168 candidates, totalling 1680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mskang/.conda/envs/server/lib/python3.9/site-packages/xgboost/data.py:173: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "  8%|▊         | 8/100 [1:37:54<18:46:01, 734.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m# pds = PredefinedSplit(np.append(-np.ones(len(x)-168), np.zeros(168)))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m gcv \u001b[39m=\u001b[39m GridSearchCV(estimator \u001b[39m=\u001b[39m XGBRegressor(seed \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m, gpu_id \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[1;32m     23\u001b[0m                                             tree_method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgpu_hist\u001b[39m\u001b[39m'\u001b[39m, predictor\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgpu_predictor\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     24\u001b[0m                    param_grid \u001b[39m=\u001b[39m grid, scoring \u001b[39m=\u001b[39m smape, cv \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m, refit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, verbose \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 27\u001b[0m gcv\u001b[39m.\u001b[39;49mfit(x, y)\n\u001b[1;32m     28\u001b[0m best \u001b[39m=\u001b[39m gcv\u001b[39m.\u001b[39mbest_estimator_\n\u001b[1;32m     29\u001b[0m params \u001b[39m=\u001b[39m gcv\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/xgboost/sklearn.py:542\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m         params\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39meval_metric\u001b[39m\u001b[39m'\u001b[39m: eval_metric})\n\u001b[0;32m--> 542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(params, train_dmatrix,\n\u001b[1;32m    543\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(), evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m    544\u001b[0m                       early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m    545\u001b[0m                       evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m    546\u001b[0m                       obj\u001b[39m=\u001b[39;49mobj, feval\u001b[39m=\u001b[39;49mfeval,\n\u001b[1;32m    547\u001b[0m                       verbose_eval\u001b[39m=\u001b[39;49mverbose, xgb_model\u001b[39m=\u001b[39;49mxgb_model,\n\u001b[1;32m    548\u001b[0m                       callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[1;32m    550\u001b[0m \u001b[39mif\u001b[39;00m evals_result:\n\u001b[1;32m    551\u001b[0m     \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m evals_result\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/xgboost/training.py:208\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mif\u001b[39;00m evals_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(callback\u001b[39m.\u001b[39mrecord_evaluation(evals_result))\n\u001b[0;32m--> 208\u001b[0m \u001b[39mreturn\u001b[39;00m _train_internal(params, dtrain,\n\u001b[1;32m    209\u001b[0m                        num_boost_round\u001b[39m=\u001b[39;49mnum_boost_round,\n\u001b[1;32m    210\u001b[0m                        evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m    211\u001b[0m                        obj\u001b[39m=\u001b[39;49mobj, feval\u001b[39m=\u001b[39;49mfeval,\n\u001b[1;32m    212\u001b[0m                        xgb_model\u001b[39m=\u001b[39;49mxgb_model, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/xgboost/training.py:75\u001b[0m, in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m# Distributed code: need to resume to this point.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m# Skip the first update if it is a recovery step.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39mif\u001b[39;00m version \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m     bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m     76\u001b[0m     bst\u001b[39m.\u001b[39msave_rabit_checkpoint()\n\u001b[1;32m     77\u001b[0m     version \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/server/lib/python3.9/site-packages/xgboost/core.py:1159\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_features(dtrain)\n\u001b[1;32m   1158\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1160\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[1;32m   1161\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[1;32m   1162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1163\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "smape = make_scorer(SMAPE, greater_is_better = False)\n",
    "\n",
    "df = pd.DataFrame(columns = ['n_estimators', 'eta', 'min_child_weight','max_depth', 'colsample_bytree', 'subsample'])\n",
    "# df = pd.read_csv('./data/xgboost/hyperparameter_xgb_nosolar.csv')\n",
    "preds = np.array([])\n",
    "\n",
    "grid = {'n_estimators' : [100], 'eta' : [0.07], 'min_child_weight' : np.arange(1, 8, 1),\n",
    "        'max_depth' : np.arange(3,9,1) , 'colsample_bytree' :np.arange(0.8, 1.0, 0.1),\n",
    "        'subsample' :np.arange(0.8, 1.0, 0.1)} # fix the eta(learning rate)\n",
    "\n",
    "# 건물 번호별로 GridSearch로 parameter 생성\n",
    "for i in tqdm(np.arange(1, 101)):\n",
    "    y = train.loc[train.num == i, 'power']\n",
    "    x = train.loc[train.num == i, ].iloc[:, 3:]\n",
    "    # 마지막 일주일 발전량을 validset으로 24시간*7일 = 168\n",
    "    y_train, y_test, x_train, x_test = temporal_train_test_split(y = y, X = x, test_size = 168)\n",
    "\n",
    "\n",
    "    # pds = PredefinedSplit(np.append(-np.ones(len(x)-168), np.zeros(168)))\n",
    "    gcv = GridSearchCV(estimator = XGBRegressor(seed = 10, gpu_id = 0,\n",
    "                                                tree_method = 'gpu_hist', predictor= 'gpu_predictor'),\n",
    "                       param_grid = grid, scoring = smape, cv = 10, refit = True, verbose = True)\n",
    "\n",
    "\n",
    "    gcv.fit(x, y)\n",
    "    best = gcv.best_estimator_\n",
    "    params = gcv.best_params_\n",
    "    print(params)\n",
    "    pred = best.predict(x_test)\n",
    "    building = 'building'+str(i)\n",
    "    print(building + '|| SMAPE : {}'.format(SMAPE(y_test, pred)))\n",
    "    preds = np.append(preds, pred)\n",
    "    df = pd.concat([df, pd.DataFrame(params, index = [0])], axis = 0)\n",
    "    df.to_csv('./data/xgboost/hyperparameter_xgb_nosolar.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = pd.read_csv('./data/xgboost/hyperparameter_xgb_nosolar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실함수를 rmse 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weighted_mse 는 모든 파라미터 학습 후에 적용해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_estimators 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted_mse 쓰는것과 안쓰는 것 차이 확인\n",
    "# weighted_mse 안쓰는 것이 더 점수 높게 나오네?? 둘다 실험해보자\n",
    "\n",
    "scores = []   # smape 값을 저장할 list\n",
    "best_it = []  # best interation을 저장할 list\n",
    "for i in tqdm(range(100)):\n",
    "    y = train.loc[train.num == i+1, 'power']\n",
    "    x = train.loc[train.num == i+1, ].iloc[:, 3:]\n",
    "    y_train, y_valid, x_train, x_valid = temporal_train_test_split(y = y, X = x, test_size = 168)\n",
    "    \n",
    "    xgb_reg = XGBRegressor(n_estimators = 1000, eta = xgb_params.iloc[i, 1], min_child_weight = xgb_params.iloc[i, 2],\n",
    "                           max_depth = xgb_params.iloc[i, 3], colsample_bytree = xgb_params.iloc[i, 4], \n",
    "                           subsample = xgb_params.iloc[i, 5], seed=10)\n",
    "    # xgb_reg.set_params(**{'objective':weighted_mse(100)}) # alpha = 100으로 고정\n",
    "    \n",
    "    xgb_reg.fit(x_train, y_train, eval_set=[(x_train, y_train), \n",
    "                                            (x_valid, y_valid)], early_stopping_rounds=300, verbose=False)\n",
    "    y_pred = xgb_reg.predict(x_valid)\n",
    "    pred = pd.Series(y_pred)   \n",
    "    \n",
    "    sm = SMAPE(y_valid, y_pred)\n",
    "    scores.append(sm)\n",
    "    best_it.append(xgb_reg.best_iteration) ## 실제 best iteration은 이 값에 +1 해주어야 함.\n",
    "    print(\"building {} || best iter : {} || smape : {}\".format(i+1, xgb_reg.best_iteration, sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜닝한 n_estimators 적용시켜서 smape 값 추출\n",
    "smape_list = []\n",
    "for i in tqdm(range(100)):\n",
    "    y = train.loc[train.num == i+1, 'power']\n",
    "    x = train.loc[train.num == i+1, ].iloc[:, 3:]\n",
    "    y_train, y_test, x_train, x_test = temporal_train_test_split(y = y, X = x, test_size = 168)\n",
    "    xgb = XGBRegressor(seed = 10,\n",
    "                      n_estimators = best_it[i], eta = xgb_params.iloc[i, 1], min_child_weight = xgb_params.iloc[i, 2],\n",
    "                      max_depth = xgb_params.iloc[i, 3], colsample_bytree = xgb_params.iloc[i, 4], subsample = xgb_params.iloc[i, 5], eval_metric = 'rmse')\n",
    "    \n",
    "    xgb.fit(x_train, y_train)\n",
    "    pred0 = xgb.predict(x_test)\n",
    "    score0 = SMAPE(y_test,pred0)\n",
    "\n",
    "    smape_list.append(score0)\n",
    "    print(\"building {} || best score : {}\".format(i+1, score0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_df = pd.DataFrame({'score':smape_list})\n",
    "plt.bar(np.arange(len(no_df))+1, no_df['score'])\n",
    "plt.plot([1,100], [5, 5], color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params['best_it'] = best_it\n",
    "xgb_params.to_csv('./data/xgboost/hyperparameter_xgb_nosolar_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best hyperparameters 불러오기\n",
    "xgb_params = pd.read_csv('./data/xgboost/hyperparameter_xgb_nosolar_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. test 전처리 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/xgboost/test_preprocessed.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([]) \n",
    "for i in tqdm(range(100)):\n",
    "    \n",
    "    y_train = train.loc[train.num == i+1, 'power']\n",
    "    x_train, x_test = train.loc[train.num == i+1, ].iloc[:, 3:], test.loc[test.num == i+1, ].iloc[:,2:]\n",
    "    x_test = x_test[x_train.columns]\n",
    "    \n",
    "    xgb = XGBRegressor(seed = 10, n_estimators = xgb_params.iloc[i, -1], eta = xgb_params.iloc[i, 1], \n",
    "                        min_child_weight = xgb_params.iloc[i, 2], max_depth = xgb_params.iloc[i, 3], \n",
    "                        colsample_bytree=xgb_params.iloc[i, 4], subsample=xgb_params.iloc[i, 5])\n",
    "\n",
    "    # if xgb_params.iloc[i,6] != 0:  # 만약 alpha가 0이 아니면 weighted_mse 사용\n",
    "    #     xgb.set_params(**{'objective':weighted_mse(xgb_params.iloc[i,6])})\n",
    "    \n",
    "    xgb.fit(x_train, y_train)\n",
    "    y_pred = xgb.predict(x_test)\n",
    "    preds = np.append(preds, y_pred)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과 시각화\n",
    "\n",
    "preds = pd.Series(preds)\n",
    "\n",
    "fig, ax = plt.subplots(100, 1, figsize=(100,200), sharex = True)\n",
    "ax = ax.flatten()\n",
    "for i in range(100):\n",
    "    train_y = train.loc[train.num == i+1, 'power'].reset_index(drop = True)\n",
    "    test_y = preds[i*168:(i+1)*168]\n",
    "    ax[i].scatter(np.arange(2040) , train.loc[train.num == i+1, 'power'])\n",
    "    ax[i].scatter(np.arange(2040, 2040+168) , test_y)\n",
    "    ax[i].tick_params(axis='both', which='major', labelsize=6)\n",
    "    ax[i].tick_params(axis='both', which='minor', labelsize=4)\n",
    "#plt.savefig('./predict_xgb.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./data/sample_submission.csv')\n",
    "submission['answer'] = preds\n",
    "submission.to_csv('./data/xgboost/submission_xgb_nosolor.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실함수를 임의로 생성한 weighted_mse 사용\n",
    "이 함수를 이용하는 것이 rmse 를 이용하는 것보다 smape 평가지표상에서 더 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### alpha를 argument로 받는 함수로 실제 objective function을 wrapping하여 alpha값을 쉽게 조정할 수 있도록 작성했습니다.\n",
    "# custom objective function for forcing model not to underestimate\n",
    "def weighted_mse(alpha = 1):\n",
    "    def weighted_mse_fixed(label, pred):\n",
    "        residual = (label - pred).astype(\"float\")\n",
    "        grad = np.where(residual>0, -2*alpha*residual, -2*residual)\n",
    "        hess = np.where(residual>0, 2*alpha, 2.0)\n",
    "        return grad, hess\n",
    "    return weighted_mse_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []   # smape 값을 저장할 list\n",
    "best_it = []  # best interation을 저장할 list\n",
    "for i in tqdm(range(100)):\n",
    "    y = train.loc[train.num == i+1, 'power']\n",
    "    x = train.loc[train.num == i+1, ].iloc[:, 3:]\n",
    "    y_train, y_valid, x_train, x_valid = temporal_train_test_split(y = y, X = x, test_size = 168)\n",
    "    \n",
    "    xgb_reg = XGBRegressor(n_estimators = 10000, eta = 0.01, min_child_weight = xgb_params.iloc[i, 2],\n",
    "                           max_depth = xgb_params.iloc[i, 3], colsample_bytree = xgb_params.iloc[i, 4], \n",
    "                           subsample = xgb_params.iloc[i, 5], seed=10)\n",
    "    xgb_reg.set_params(**{'objective':weighted_mse(100)}) # alpha = 100으로 고정\n",
    "    \n",
    "    xgb_reg.fit(x_train, y_train, eval_set=[(x_train, y_train), \n",
    "                                            (x_valid, y_valid)], early_stopping_rounds=300, verbose=False)\n",
    "    y_pred = xgb_reg.predict(x_valid)\n",
    "    pred = pd.Series(y_pred)   \n",
    "    \n",
    "    sm = SMAPE(y_valid, y_pred)\n",
    "    scores.append(sm)\n",
    "    best_it.append(xgb_reg.best_iteration) ## 실제 best iteration은 이 값에 +1 해주어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = []\n",
    "smape_list = []\n",
    "for i in tqdm(range(100)):\n",
    "    y = train.loc[train.num == i+1, 'power']\n",
    "    x = train.loc[train.num == i+1, ].iloc[:, 3:]\n",
    "    y_train, y_test, x_train, x_test = temporal_train_test_split(y = y, X = x, test_size = 168)\n",
    "    xgb = XGBRegressor(seed = 10,\n",
    "                      n_estimators = best_it[i], eta = 0.01, min_child_weight = xgb_params.iloc[i, 2],\n",
    "                      max_depth = xgb_params.iloc[i, 3], colsample_bytree = xgb_params.iloc[i, 4], subsample = xgb_params.iloc[i, 5])\n",
    "    \n",
    "    xgb.fit(x_train, y_train)\n",
    "    pred0 = xgb.predict(x_test)\n",
    "    best_alpha = 0\n",
    "    score0 = SMAPE(y_test,pred0)\n",
    "    \n",
    "    for j in range(1, 100, 2):\n",
    "        xgb = XGBRegressor(seed = 10,\n",
    "                      n_estimators = best_it[i], eta = 0.01, min_child_weight = xgb_params.iloc[i, 2],\n",
    "                      max_depth = xgb_params.iloc[i, 3], colsample_bytree = xgb_params.iloc[i, 4], subsample = xgb_params.iloc[i, 5])\n",
    "        xgb.set_params(**{'objective' : weighted_mse(j)})\n",
    "    \n",
    "        xgb.fit(x_train, y_train)\n",
    "        pred1 = xgb.predict(x_test)\n",
    "        score1 = SMAPE(y_test, pred1)\n",
    "        if score1 < score0:\n",
    "            best_alpha = j\n",
    "            score0 = score1\n",
    "    \n",
    "    alpha_list.append(best_alpha)\n",
    "    smape_list.append(score0)\n",
    "    print(\"building {} || best score : {} || alpha : {}\".format(i+1, score0, best_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params['alpha'] = alpha_list\n",
    "xgb_params['best_it'] = best_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([]) \n",
    "for i in tqdm(range(100)):\n",
    "    \n",
    "    y_train = train.loc[train.num == i+1, 'power']\n",
    "    x_train, x_test = train.loc[train.num == i+1, ].iloc[:, 3:], test.loc[test.num == i+1, ].iloc[:,2:]\n",
    "    x_test = x_test[x_train.columns]\n",
    "    \n",
    "    xgb = XGBRegressor(seed = 10, n_estimators = xgb_params.iloc[i, -1], eta = xgb_params.iloc[i, 1], \n",
    "                        min_child_weight = xgb_params.iloc[i, 2], max_depth = xgb_params.iloc[i, 3], \n",
    "                        colsample_bytree=xgb_params.iloc[i, 4], subsample=xgb_params.iloc[i, 5])\n",
    "\n",
    "    if xgb_params.iloc[i,6] != 0:  # 만약 alpha가 0이 아니면 weighted_mse 사용\n",
    "        xgb.set_params(**{'objective':weighted_mse(xgb_params.iloc[i,6])})\n",
    "    \n",
    "    xgb.fit(x_train, y_train)\n",
    "    y_pred = xgb.predict(x_test)\n",
    "    preds = np.append(preds, y_pred)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과 시각화\n",
    "\n",
    "preds = pd.Series(preds)\n",
    "\n",
    "fig, ax = plt.subplots(100, 1, figsize=(100,200), sharex = True)\n",
    "ax = ax.flatten()\n",
    "for i in range(100):\n",
    "    train_y = train.loc[train.num == i+1, 'power'].reset_index(drop = True)\n",
    "    test_y = preds[i*168:(i+1)*168]\n",
    "    ax[i].scatter(np.arange(2040) , train.loc[train.num == i+1, 'power'])\n",
    "    ax[i].scatter(np.arange(2040, 2040+168) , test_y)\n",
    "    ax[i].tick_params(axis='both', which='major', labelsize=6)\n",
    "    ax[i].tick_params(axis='both', which='minor', labelsize=4)\n",
    "#plt.savefig('./predict_xgb.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./data/sample_submission.csv')\n",
    "submission['answer'] = preds\n",
    "submission.to_csv('./data/xgboost/submission_xgb_nosolor_wmse.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "server",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
